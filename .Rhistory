trans_data <- preProcess(as.data.frame(data), method=c("range"))
View(trans_data)
norm_data <- predict(trans_data, as.data.frame(data))
View(norm_data)
print(norm_data)
# Standardization
# Create data
data <- c(244,753,596,645,874,141,639,465,999,654)
# standardizing data
std_data <- as.data.frame(scale(data))
View(std_data)
print(std_data)
# Simple linear regression
#data of height
x <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)
#data of weight
y <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)
# Apply the lm() function
relation <- lm(y ~ x) #used to fit linear models
View(relation)
print(relation)
# Find weight of a person with height 170
a <- data.frame(x = 170)
View(a)
result <- predict(relation, a)
print(round(result, digit = 2))
# Plot the chart
plot(y, x, col="blue", main="Height & Weight Regression", abline(lm(x~y)), cex=1.3, pch=16, xlab="Weight in Kg", ylab="Height in cm")
###################################################################
# Example case study
data <- read.csv("C:/Users/USER/OneDrive/Documents/Undergraduate/Year 2/SEM 2/Introduction to Data Science/Database/Week 7_income.data.csv")
View(data)
data.train <- data[1:398,]
data.test <- data[399:498,]
#apply the lm() function
relation <- lm(happiness ~ income, data = data.train)
print(relation)
#prediction
a <- dayta.frame(x = data.test$income)
#prediction
a <- data.frame(x = data.test$income)
View(a)
colnames(a) <- "income"
result <- predit(relation, a)
result <- predict(relation, a)
print(result)
#plot the chart
plot(data.test$income, data.test$happiness, col="red", abline(lm(happiness~income), data=data.train), cex=1.3, pch=16, xlab="income", ylab="happiness")
#plot the chart
plot(data.test$income, data.test$happiness, col="red", abline(lm(happiness~income, data=data.train)), cex = 1.3, pch = 16, xlab = "income", ylab = "happiness")
#performance measurement
actuals_preds <- data.frame(cbind(actuals=data.test$happiness, predicteds=result))
View(actuals_preds)
#make actuals_predicteds dataframe
correlation_accuracy <- cor(actuals_preds)
head(actuals_preds)
#measuring errors in MAPE
mapa <- mean(abs((actuals_preds$predocteds - actuals_preds$actuals)) / actuals_pprds$actuals) * 100
#measuring errors in MAPE
mapa <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals)) / actuals_pprds$actuals) * 100
#measuring errors in MAPE
mapa <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals)) / actuals_preds$actuals) * 100
paste("The error - MAPE is:", round(mape, digit = 2), "%")
#measuring errors in MAPE
mape <- mean(abs((actuals_preds$predicteds - actuals_preds$actuals)) / actuals_preds$actuals) * 100
paste("The error - MAPE is:", round(mape, digit = 2), "%")
# Import external dataset
heart.data <- read.csv("C:/Users/USER/OneDrive/Documents/Undergraduate/Year 2/SEM 2/Introduction to Data Science/Database/Week 8_heart.data.csv")
head(heart.data)
str(head.data)
str(heart.data)
# Model the MLR (machine learning R)
heart.disease.lm <- lm(heart.disease ~ biking + smoking, data = heart.data)
View(heart.disease.lm)
# Investigate the properties of the model
summary(heart.disease.lm)
# Split data into train and test sets
data.train <- heart.data[1:398,]
data.test <- heart.data[399:498,]
# Modelling
heart.disease.lm <- lm(heart,disease ~ biking + smoking, data = data.train)
# Modelling
heart.disease.lm <- lm(heart.disease ~ biking + smoking, data = data.train)
View(heart.disease.lm)
summary(heart.disease.lm)
# Prediction
a <- data.frame(biking=data.test$biking, smoking=data.test$smoking)
View(a)
result <- predict(heart.disease.lm, a)
# Performance measurement
actuals_preds <- data.frame(cbing(actuals=data.test$heart.disease, predicteds=result))
# Performance measurement
actuals_preds <- data.frame(cbind(actuals=data.test$heart.disease, predicteds=result))
correlation_accuracy <- cor(actuals_preds)
head(actuals_preds)
mape <- mean(abs(actuals_preds$predicteds - actual_preds$actuals) / actuals_preds$actual) * 100
mape <- mean(abs(actuals_preds$predicteds - actuals_preds$actuals) / actuals_preds$actual) * 100
paste("the error - MAPE is:", round(mape, digit=2), "%")
###################################################################
# CART for Classification prediction [predicting iris species]
#import library
install.packages("ggformula")
install.packages("tree")
library(ggformula)
library(tree)
library(dplyr)
install.packages("dplyr")
install.packages("dplyr")
library(ggformula)
library(tree)
library(dplyr)
#explore the dataset
head(iris)
tail(iris)
#visualize the data
gf_point(Sepal.Lengtg ~ Sepal.Width, data = iris, color = ~ Specisies) %>% gf_labs(title = " Figure 1: Scatterplot of Iris Data")
#visualize the data
gf_point(Sepal.Lengtg ~ Sepal.Width, data = iris, color = ~ Species) %>% gf_labs(title = " Figure 1: Scatterplot of Iris Data")
#visualize the data
gf_point(Sepal.Length ~ Sepal.Width, data = iris, color = ~ Species) %>% gf_labs(title = " Figure 1: Scatterplot of Iris Data")
#create the classification tree
treeall <- tree(Species ~ ., data = iris)
View(treeall)
#visualize the tree
plot(treeall)
text(treeall, cex = 0.5)
#create a summary table (called a confusion matrix)
#output is the class instead of the probability
tree.pred <- predict(treeall, iris, type = "class")
table(tree.pred, iris$Species)
###################################################################
# CART for Regression prediction [Hitters dataset]
#import library
install.packages("ISLR")
install.packages("rpart")
install.packages("rpart.plot")
library(ISLR) #contains Hitters dataset
library(rpart) #for fitting decision trees
library(rpart.plot) #for plotting decision trees
#view dataset
data(Hitters)
head(Hitters)
View(Hitters)
#checking for NAs and removal
sum(is.na(Hitters$Salary))
df_hitters <- Hitters
df_hitters <- na.omit(df_hitters)
View(df_hitters)
View(Hitters)
#divide the data into train and test sets, only 13 observations for testing
df_train <- df_hitters[1:250,]
df_test <- df_hitters[252:263,]
#build the initial tree
tree <-rpart(Salary ~ Years + HmRum, data = df_train, control = rpart.control(cp = .001))
View(data.train)
View(df_train)
#build the initial tree
tree <-rpart(Salary ~ Years + HmRun, data = df_train, control = rpart.control(cp = .001))
View(tree)
#view result
printco(tree)
#view result
printcp(tree)
#identify best cp value to use
best <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
#product a pruned tree based on the best cp value
pruned_tree1 <- prune(tree, cp=best)
View(pruned_tree1)
#plot the pruned tree
prp(pruned_tree1,
faclen = 0, #use full names for factor labels
extra = 1, #display number of observation for each terminal node
roundint = F, #don't round to integers in output
digits = 5) #display 5 decimal place in output
#prediction result
res <- predict(pruned_tree1, newdata = df_test[c("Years", "HmRun")])
print(res)
#getting the actual Salary values
act <- Hitters$Salary[250:263]
#calculating the errors
MAE.trees <- mean(abs(res - act), na.rm = T) #remove missing value
#calculating the errors
MAE.tree1 <- mean(abs(res-act),na.rm=T)# remove missing values
#plot the pruned tree
prp(pruned_tree1,
faclen = 0, #use full names for factor labels
extra = 1, #display number of obs. for each terminal node
roundint = F, #don't round to integers in output
digits = 5) #display 5 decimal places in output
#prediction result
res <- predict(pruned_tree1, newdata = df_test[c("Years", "HmRun")])
print(res)
#getting the actual Salary values
act <- Hitters$Salary[250:263]
#calculating the errors
MAE.tree1 <- mean(abs(res-act),na.rm=T)# remove missing values
print(MAE.tree1)
library(ISLR) #contains Hitters dataset
library(rpart) #for fitting decision trees
library(rpart.plot) #for plotting decision trees
#view dataset
data(Hitters)
head(Hitters)
#checking for NAs and removal
sum(is.na(Hitters$Salary))
df_hitters <- Hitters
df_hitters <- na.omit(df_hitters)
#divide the data into train and test sets, only 13 observations for testing
df_train <- df_hitters[1:250,]
df_test <- df_hitters[252:263,]
#build the initial tree
tree <-rpart(Salary ~ Years + HmRun, data = df_train, control = rpart.control(cp = .001))
#view result
printcp(tree)
#identify best cp value to use
best <- tree$cptable[which.min(tree$cptable[, "xerror"]), "CP"]
#product a pruned tree based on the best cp value
pruned_tree1 <- prune(tree, cp = best)
#plot the pruned tree
prp(pruned_tree1,
faclen = 0, #use full names for factor labels
extra = 1, #display number of obs. for each terminal node
roundint = F, #don't round to integers in output
digits = 5) #display 5 decimal places in output
#prediction result
res <- predict(pruned_tree1,newdata = df_test[c("Years","HmRun")])
print(res)
#getting the actual Salary values
act <- Hitters$Salary[250:263]
#calculating the errors
MAE.tree1 <- mean(abs(res-act),na.rm=T)# remove missing values
print(MAE.tree1)
# K-Means
#before clustering
x = c(5.1,4.9,4.7,4.6,5,5.4,4.6,5,4.4,4.9,5.3,5.2,5.2,5.3,4.4,4.5,4.5,4.4)
y = c(3.5,3,3.2,3.1,3.6,3.9,3.4,3.4,2.9,3.1,3.6,3.8,3.6,3.7,3.1,3.6,3.5,3.2)
plot(x, y, col = "blue", pch = 19, cex = 2)
#after clustering
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers = 3)
plot(x, y, col = kmeansObj$cluster, pch = 19, cex = 2)
# Activity 1
data(Theoph)
force(Theoph)
# Activity 1
data(Theoph)
x = Theoph$Dose
y = Theoph$Wt
model <- lm(x ~ y)
View(model)
print(model)
print(summary(model))
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
model <- lm(y ~ x)
print(model)
print(summary(model))
x
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
#plot visualize
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
x
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
scatter.smooth(x, y, main = "Dose ~ Wt", xlab = "Dose", ylab = "Wt")
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
scatter.smooth(x, y, main = "Dose ~ Wt", xlab = "Dose", ylab = "Wt")
scatter.smooth(y, x, main = "Dose ~ Wt", ylab = "Dose", xlab = "Wt")
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
scatter.smooth(x, y, main = "Dose ~ Wt", xlab = "Dose", ylab = "Wt")
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Dose", ylab = "Wt")
scatter.smooth(y, x, main = "Dose ~ Wt", ylab = "Dose", xlab = "Wt")
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
# Activity 1
data(Theoph)
x = Theoph$Wt
y = Theoph$Dose
model <- lm(y ~ x)
print(model)
print(summary(model))
#plot visualize
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
scatter.smooth(y, x, main = "Wt ~ Dose", ylab = "Dose", xlab = "Wt")
x=cars$speed
y=cars$dist
model1 <- lm(y~x)
print(model1)
print(summary(model1))
plot(y,x,main = "Distance & Speed",abline(lm(speed~dist,
data=cars)),xlab = "Distance",ylab = "Speed")
scatter.smooth(y,x, main="Dist ~ Speed",xlab = "Distance",ylab =
"Speed")
#plot visualize
plot(y, x, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
y
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
x = Theoph$Wt #independent variables
y = Theoph$Dose #dependent variables
relation <- lm(y ~ x)
print(relation)
print(summary(relation))
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", ylab = "Dose", xlab = "Wt")
View(relation)
View(Theoph)
#plot visualize
plot(x, y, main = "The dose of theophyline administered orally to the subject (mg/kg)", xlab = "Wt", ylab = "Dose")
scatter.smooth(x, y, main = "Dose ~ Wt", xlab = "Wt", ylab = "Dose")
#predict new value
newWt <- data.frame(c(90, 95, 100))
result <- predict(relation, newWt)
#predict new value
newWt <- data.frame(x = c(90, 95, 100))
result <- predict(relation, newWt)
print(result)
# Activity 2
data(ChickWeight)
head(ChickWeight)
install.packages("e1071")
install.packages("caTools")
install.packages("class")
# Loading package
library(e1071)
library(caTools)
library(class)
###################################################################
# K-NN Classifier
# Loading package
library(e1071)
library(caTools)
library(class)
# Loading data
data(iris)
head(iris)
# Splitting data into train and test data
# Splitting data into train and test data
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
View(train_cl)
test_cl <- subset(iris, split == "FALSE")
# Feature Scaling
train_scale <- scale(train_cl[, 1:4])
View(train_scale)
test_scale <- scale(test_cl[, 1:4])
View(test_scale)
# Splitting data into train and test data
# Fitting KNN Model to training dataset
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 1)
print(classifier_knn)
# Evaluating the Model
# Confusion Matrix
cm <- table(test_cl$Species, classifier_knn)
print(cm)
# Calculate out of Sample error
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# Choosing the K value
# K = 3
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 3)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# K = 5
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 5)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# K = 7
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 7)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# K = 7
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 7)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# K = 19
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 19)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
# Activity 2
data(ChickWeight)
head(ChickWeight)
# Loading package
library(e1071)
library(caTools)
library(class)
#Splitting data into train and test
split <- sample.split(Chick/weight, SplitRatio = 0.7)
#Splitting data into train and test
split <- sample.split(Chickweight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Feature scaling
train_scale <- scale(train_data[, 1:4])
test_scale <- scale(test_data[, 1:4])
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Feature scaling
train_scale <- scale(train_data[, 1:4])
test_scale <- scale(test_data[, 1:4])
View(test_data)
View(train_data)
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.8)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Feature scaling
train_scale <- scale(train_data[, 1:4])
test_scale <- scale(test_data[, 1:4])
#Fitting KNN model to training dataset
classifier_knn <- knn(train = train_scale,
test = test_scale,
data = train_data$Diet,
k = 1)
# Loading data
data(iris)
head(iris)
# Splitting data into train and test data
# Splitting data into train and test data
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
# Feature Scaling
train_scale <- scale(train_cl[, 1:4])
test_scale <- scale(test_cl[, 1:4])
View(train_scale)
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
View(test_data)
#Feature scaling
train_scale <- scale(train_data[, 1:4])
#Feature scaling
train_scale <- scale(as.numeric(train_data[, 1:4]))
#Feature scaling
train_scale <- scale(train_data[, 1:4], is.numeric)
as.numeric(train_data)
#Feature scaling
train_scale <- scale(train_data[, 1:4])
#Feature scaling
train_scale <- as.numeric(scale(train_data[, 1:4]))
#Feature scaling
train_scale[, 1:4] <- scale(train_data)
# Activity 2
data(ChickWeight)
head(ChickWeight)
# Activity 2
data(ChickWeight)
head(ChickWeight)
View(ChickWeight)
# Loading package
library(e1071)
library(caTools)
library(class)
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Feature scaling
train_scale[, 1:4] <- scale(train_data)
test_scale <- scale(test_data[, 1:4])
as.numeric(ChickWeight)
as.numeric(unlist(ChickWeight))
# Loading package
library(e1071)
library(caTools)
library(class)
#Splitting data into train and test
split <- sample.split(ChickWeight, SplitRatio = 0.7)
train_data <- subset(ChickWeight, split == "TRUE")
test_data <- subset(ChickWeight, split == "TRUE")
#Feature scaling
train_scale <- scale(train_data[, 1:4])
test_scale <- scale(test_data[, 1:4])
#Fitting KNN model to training dataset
classifier_knn <- knn(train = train_scale,
test = test_scale,
data = train_data$Diet,
k = 1)
print(classifier_knn)
